{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB6fohWzEebb"
      },
      "source": [
        "# WordToVec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tPdJFKefHVtA",
        "outputId": "1bcec452-4e99-46bd-da65-706ec777e442"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==1.5.3 gensim==4.3.1 nltk==3.8.1 scipy==1.9.3 tensorflow==2.15.0 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6ZbwZhcoMSM"
      },
      "outputs": [],
      "source": [
        "data1 = pd.read_csv('../BigBasket Products.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "gqZD5M7KoTjM",
        "outputId": "3f54a6b9-3059-4562-a825-c1e9b32c764a"
      },
      "outputs": [],
      "source": [
        "data1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "Im3G4VRao_vn",
        "outputId": "a0ce22ec-7e64-4d4b-d8b9-631986ae3e68"
      },
      "outputs": [],
      "source": [
        "data1.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCRs9WoZmdgy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('BigBasket Products.csv')\n",
        "\n",
        "rmv_spc = lambda a: a.strip()\n",
        "get_list = lambda a: list(map(rmv_spc, re.split(r'& |, |\\*|\\n', a)))\n",
        "\n",
        "for col in ['category', 'sub_category', 'type']:\n",
        "    df[col] = df[col].apply(get_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44PA-kqvmhFi"
      },
      "outputs": [],
      "source": [
        "def cleaner(x):\n",
        "    if isinstance(x, list):\n",
        "        return [i.lower().replace(' ', '') for i in x]\n",
        "    elif isinstance(x, str):\n",
        "        return x.lower().replace(' ', '')\n",
        "    else:\n",
        "        return ''\n",
        "for col in ['category', 'sub_category', 'type', 'brand']:\n",
        "    df[col] = df[col].apply(cleaner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUUh1mylmpOg"
      },
      "outputs": [],
      "source": [
        "def create_soup(x):\n",
        "    description_text = ''\n",
        "    if isinstance(x['description'], (list, tuple)):\n",
        "        description_text = ' '.join(x['description'])\n",
        "    elif isinstance(x['description'], str):\n",
        "        description_text = x['description']\n",
        "\n",
        "    return ' '.join(x['category']) + ' ' + \\\n",
        "           ' '.join(x['sub_category']) + ' ' + \\\n",
        "           x['brand'] + ' ' + \\\n",
        "           ' '.join(x['type']) + ' ' + description_text\n",
        "\n",
        "df['soup'] = df.apply(create_soup, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(lower=True, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['soup'])\n",
        "seqs = tokenizer.texts_to_sequences(df['soup'])\n",
        "maxlen = max(len(s) for s in seqs)\n",
        "X = pad_sequences(seqs, maxlen=maxlen, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embed_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encoder-decoder Architecture\n",
        "inp = Input(shape=(maxlen,), name='encoder_input')\n",
        "x = Embedding(vocab_size, embed_dim, input_length=maxlen, name='emb')(inp)\n",
        "\n",
        "x = Bidirectional(LSTM(128, return_sequences=True), name='bilstm_1')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Bidirectional(LSTM(64, return_sequences=True), name='bilstm_2')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = GlobalMaxPooling1D(name='pool')(x)\n",
        "\n",
        "# Bottleneck layer\n",
        "encoded = Dense(256, activation='relu', name='bottleneck_1')(x)\n",
        "encoded = Dropout(0.3)(encoded)\n",
        "encoded = Dense(128, activation='relu', name='bottleneck_2')(encoded)\n",
        "\n",
        "decoded = Dense(vocab_size, activation='softmax', name='decoder_output')(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "autoencoder = Model(inp, decoded, name='autoencoder')\n",
        "autoencoder.compile(\n",
        "    optimizer=Adam(1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = X[:, 0]\n",
        "autoencoder.fit(X, y, epochs=10, batch_size=128, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = Model(inp, encoded, name='encoder')\n",
        "encoder.save('bb_encoder.h5')\n",
        "\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer.to_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding\n",
        "\n",
        "Adding the embedding column to every row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('tokenizer.json') as f:\n",
        "    tok_json = f.read()\n",
        "tokenizer = tokenizer_from_json(tok_json)\n",
        "encoder = load_model('bb_encoder.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seqs = tokenizer.texts_to_sequences(df['soup'])\n",
        "maxlen = encoder.input_shape[1]\n",
        "X = pad_sequences(seqs, maxlen=maxlen, padding='post')\n",
        "embeddings = encoder.predict(X, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['embedding'] = [e.tolist() for e in embeddings]\n",
        "df.to_csv('BigBasket_Products_emb.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA5Z26RimDsB"
      },
      "source": [
        "# Test (main.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGMp6pddnl9H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from urllib.parse import unquote\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lvU-6ZrnoM2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('BigBasket_Products_emb.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOPaOy_6nqEo"
      },
      "outputs": [],
      "source": [
        "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzsREinwnrJ_"
      },
      "outputs": [],
      "source": [
        "embeddings = np.array(df['embedding'].to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6m5NXbbns6Q"
      },
      "outputs": [],
      "source": [
        "cosine_sim = cosine_similarity(embeddings, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvww3nwznuX3"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index(drop=True)\n",
        "indices = pd.Series(df.index, index=df['product']).drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_list_fields(row, list_fields):\n",
        "    for field in list_fields:\n",
        "        if isinstance(row[field], str) and row[field].startswith(\"[\"):\n",
        "            try:\n",
        "                row[field] = ast.literal_eval(row[field])\n",
        "            except:\n",
        "                row[field] = []\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqAIR4epnw5x"
      },
      "outputs": [],
      "source": [
        "def get_recommendations(product_name, topn=10):\n",
        "    try:\n",
        "        decoded = unquote(product_name)\n",
        "        logger.info(f\"Getting recommendations for: {decoded}\")\n",
        "        idx = indices[decoded]\n",
        "        if isinstance(idx, pd.Series):\n",
        "            idx = idx.iloc[0]\n",
        "    except KeyError:\n",
        "        logger.warning(f\"Product not found: {product_name}\")\n",
        "        return None\n",
        "\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1: topn+1]\n",
        "    rec_idxs = [i for i, _ in sim_scores]\n",
        "\n",
        "    exclude_keys = {'embedding', 'rating', 'soup', 'index'}\n",
        "    list_fields = ['category', 'sub_category', 'type']\n",
        "\n",
        "    recommendations = []\n",
        "    for _, row in df.iloc[rec_idxs].copy().iterrows():\n",
        "        row = parse_list_fields(row, list_fields)\n",
        "        filtered = {k: v for k, v in row.items() if k not in exclude_keys}\n",
        "        recommendations.append(filtered)\n",
        "\n",
        "    logger.info(f\"Found {len(recommendations)} recommendations for {decoded}\")\n",
        "    return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fflqEQIxoFkM"
      },
      "outputs": [],
      "source": [
        "def get_random_products(n=15):\n",
        "    logger.info(f\"Fetching {n} random products\")\n",
        "    sample_df = df.sample(n=n).copy()\n",
        "    list_fields = ['category', 'sub_category', 'type']\n",
        "    sample_df = sample_df.apply(lambda row: parse_list_fields(row, list_fields), axis=1)\n",
        "    logger.info(\"Random products fetched successfully\")\n",
        "    return sample_df.to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhYPVdkQoFg1",
        "outputId": "9a53b041-0183-4860-e6f2-1fbfc6baee62"
      },
      "outputs": [],
      "source": [
        "get_recommendations('Whisky Glass - Elegan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnFcb37foFeC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXEVb9r7oEyG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hq2Ux6Jjojt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-IVEZiQSpJW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
